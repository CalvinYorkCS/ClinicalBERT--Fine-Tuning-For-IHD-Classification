{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":186,"status":"ok","timestamp":1733372555138,"user":{"displayName":"Calvin York","userId":"14741924443884502641"},"user_tz":360},"id":"HYo29aqUnlc_"},"outputs":[],"source":["# pip install transformers torch pandas scikit-learn"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1216,"status":"ok","timestamp":1733372556352,"user":{"displayName":"Calvin York","userId":"14741924443884502641"},"user_tz":360},"id":"63j63Q1in5pf","outputId":"64987fff-80be-45e3-9be5-59e59116e710"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1480,"status":"ok","timestamp":1733372557830,"user":{"displayName":"Calvin York","userId":"14741924443884502641"},"user_tz":360},"id":"EcLxymVdof3y"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","path = \"drive/MyDrive/HealthML/final_project_files/\"\n","df = pd.read_csv(path + \"processed_notes.csv\")\n","\n","# Split the data into train, validation, and test sets\n","train_val, test = train_test_split(df, test_size=0.1, random_state=42)\n","train, val = train_test_split(train_val, test_size=0.1, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"NmiPfhWSssZg"},"source":["# Model Preparation"]},{"cell_type":"markdown","metadata":{"id":"rqEEIP1vs4BQ"},"source":["Load the ClinicalBERT model with a classification head from the Hugging Face transformers library: https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT?text=Paris+is+the+%5BMASK%5D+of+France."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IZBonH8Xste7","executionInfo":{"status":"ok","timestamp":1733372566819,"user_tz":360,"elapsed":8991,"user":{"displayName":"Calvin York","userId":"14741924443884502641"}},"outputId":"a26ce209-e078-4668-964b-7d566bc31e4e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification, logging\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Suppress info messages from transformers (optional, not necessary)\n","logging.set_verbosity_warning()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","# Load model with a specific configuration for binary classification (I could be wrong here if anyone can check it)\n","model = BertForSequenceClassification.from_pretrained(\n","    \"emilyalsentzer/Bio_ClinicalBERT\",\n","    num_labels=2,\n","    ignore_mismatched_sizes=True  # This will suppress the warnings about mismatch sizes\n",")\n","model.to(device)\n","\n","# Class for handle tokenization\n","class NotesDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = int(self.labels[idx])\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',  # Ensure all sequences are padded to the same length\n","            truncation=True,  # Ensure that sequences longer than model max are truncated\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(df, tokenizer, batch_size, max_len=512):\n","    ds = NotesDataset(\n","        texts=df.TEXT.to_numpy(),\n","        labels=df.Label.to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        num_workers=2\n","    )\n","\n","batch_size = 32\n","train_data_loader = create_data_loader(train, tokenizer, batch_size)\n","val_data_loader = create_data_loader(val, tokenizer, batch_size)\n","test_data_loader = create_data_loader(test, tokenizer, batch_size)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MlbxRd_ktLeR","executionInfo":{"status":"ok","timestamp":1733372566819,"user_tz":360,"elapsed":5,"user":{"displayName":"Calvin York","userId":"14741924443884502641"}}},"outputs":[],"source":["from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","total_steps = len(train_data_loader) * 10  # 10 is the number of epochs\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","\n","def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","\n","    total_batches = len(data_loader)\n","    # print(total_batches)\n","\n","    ### TODO: Something may be wrong here\n","    for step, d in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Training\", position=0, leave=True):\n","        # print(\"step: \", step)\n","        # print(\"d: \", d)\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        # print(\"input_ids: \", input_ids)\n","        # print(\"attention_mask: \", attention_mask)\n","        # print(\"labels: \", labels)\n","\n","        model.zero_grad()\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        # print('outputs: ', outputs)\n","\n","        loss = outputs.loss\n","        logits = outputs.logits\n","        _, preds = torch.max(logits, dim=1)\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Print progress every 10 batches (just want to check progres here)\n","        if (step + 1) % 10 == 0 or step == total_batches - 1:\n","            print(f'Batch {step + 1}/{total_batches}, Loss: {loss.item():.4f}')\n","\n","    average_loss = np.mean(losses)\n","    accuracy = correct_predictions.double() / n_examples\n","    return accuracy, average_loss"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"blFPRosltROT","colab":{"base_uri":"https://localhost:8080/","height":555},"executionInfo":{"status":"error","timestamp":1733372824358,"user_tz":360,"elapsed":257543,"user":{"displayName":"Calvin York","userId":"14741924443884502641"}},"outputId":"f1de33d4-c3f0-459f-893f-5c724af4da1a"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["Training:   1%|          | 10/1421 [00:29<1:21:08,  3.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 10/1421, Loss: 0.5516\n"]},{"output_type":"stream","name":"stderr","text":["Training:   1%|▏         | 20/1421 [00:57<1:19:00,  3.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 20/1421, Loss: 0.7019\n"]},{"output_type":"stream","name":"stderr","text":["Training:   2%|▏         | 30/1421 [01:25<1:17:12,  3.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 30/1421, Loss: 0.5910\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 40/1421 [01:53<1:18:02,  3.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 40/1421, Loss: 0.6910\n"]},{"output_type":"stream","name":"stderr","text":["Training:   4%|▎         | 50/1421 [02:21<1:17:39,  3.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 50/1421, Loss: 0.5654\n"]},{"output_type":"stream","name":"stderr","text":["Training:   4%|▍         | 60/1421 [02:49<1:16:23,  3.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 60/1421, Loss: 0.5270\n"]},{"output_type":"stream","name":"stderr","text":["Training:   5%|▍         | 70/1421 [03:17<1:16:03,  3.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 70/1421, Loss: 0.5099\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▌         | 80/1421 [03:46<1:15:44,  3.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 80/1421, Loss: 0.5747\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▋         | 90/1421 [04:14<1:14:20,  3.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Batch 90/1421, Loss: 0.5009\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▋         | 91/1421 [04:16<1:02:35,  2.82s/it]\n","  0%|          | 0/10 [04:17<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6bc960d90bac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     train_acc, train_loss = train_epoch(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-389edada3ddd>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# print(\"step: \", step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(\"d: \", d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in tqdm(range(10)):\n","    print(f'Epoch {epoch + 1}')\n","    train_acc, train_loss = train_epoch(\n","        model,\n","        train_data_loader,\n","        optimizer,\n","        device,\n","        scheduler,\n","        len(train)  # Make sure 'train' contains the correct number of samples\n","    )\n","    print(f'Train loss {train_loss:.4f}, Accuracy {train_acc:.4f}')"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMeF20zYrNVeOp+8UvfS9wK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}